{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IT-Lab Kubernetes Collection","text":"<p>Welcome to the IT-Lab Kubernetes Collection. This is a collection of Kubernetes applications and tools. It also contains the <code>Basics</code> to create a fully functional Kubernetes cluster.</p>"},{"location":"#navigation","title":"Navigation","text":"<p>The <code>Top-Bar Navigation</code> provides a \"guided\" tour through the documentation. It is recommended to <code>follow the order</code> of the navigation.</p> <p>So lets start to create a cluster.</p>"},{"location":"applications/","title":"Application","text":"<p>We will deploy the following applications:</p> <ul> <li>Monitoring with Prometheus and Grafana</li> <li>Some Databases</li> <li>Custom Apllications</li> </ul>"},{"location":"applications/docker-registry/","title":"Docker Registry","text":"<pre><code>echo \"192.168.178.104 registry registry.k3s.test\" &gt;&gt; /etc/hosts\n</code></pre> <pre><code>ctr image pull --platform linux/amd64 docker.io/rancher/coredns-coredns:1.6.3\nctr content fetch --platform linux/amd64 docker.io/rancher/coredns-coredns:1.6.3\nctr image tag docker.io/rancher/coredns-coredns:1.6.3 registry.k3s.test:5000/test\nctr image push --platform linux/amd64 --plain-http registry.k3s.test:5000/test\ncurl http://registry.k3s.test:5000/v2/_catalog\n    =&gt; {\"repositories\":[\"test\"]}\n</code></pre>"},{"location":"applications/keda/","title":"KEDA","text":"<p>KEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on the number of events needing to be processed.</p>"},{"location":"applications/keda/#install-in-cluster","title":"Install in Cluster","text":"<pre><code>helm repo add kedacore https://kedacore.github.io/charts\nhelm repo update\nhelm install keda kedacore/keda --namespace keda --create-namespace\n</code></pre>"},{"location":"applications/keda/#scalers","title":"Scalers","text":"<p>KEDA Scalers</p>"},{"location":"applications/keda/#prometheus","title":"Prometheus","text":"<pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: whoami-ns\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: instrumented\n  triggers:\n    - type: prometheus\n      metadata:\n        serverAddress: http://prometheus.monitoring.svc.cluster.local:9090\n        metricName: http_request_total\n        query: http_requests_total{code=\"200\",handler=\"prometheus\",method=\"get\", service=\"instrumented-service\"}\nthreshold: '50'\nidleReplicaCount: 0\nminReplicaCount: 3\nmaxReplicaCount: 10\n</code></pre>"},{"location":"applications/keda/#cpu","title":"CPU","text":"<pre><code>apiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: prometheus-scaledobject\n  namespace: whoami-ns\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: instrumented\n  triggers:\n    - type: cpu\n      metricType: Utilization # Allowed types are 'Utilization' or 'AverageValue'\nmetadata:\n        value: '180'\nidleReplicaCount: 0\nminReplicaCount: 3\nmaxReplicaCount: 10\n</code></pre>"},{"location":"applications/openfaas/","title":"OpenFaaS","text":"<p>OpenFaaS is a framework for building serverless functions with Docker and Kubernetes which has first class support for metrics. Any process can be packaged as a function enabling you to consume a range of web events without repetitive boiler-plate coding.</p>"},{"location":"applications/openfaas/#deploy","title":"Deploy","text":"<p>Use <code>make install</code> in the <code>OpenFaaS</code> folder to install in the Cluster.</p>"},{"location":"applications/openfaas/#get-password","title":"Get Password","text":"<pre><code>PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo)\necho \"OpenFaaS admin password: ${PASSWORD}\"\n</code></pre> <p>Add the password to the <code>~/.bashrc</code> (or <code>~/.zshrc</code>) file:</p> <pre><code>export OPENFAAS_URL=http://openfaas.k3s.test:8080\n</code></pre> <p>Reload the shell. Login to OpenFaaS:</p> <pre><code>echo -n $PASSWORD | faas-cli login --username admin --password-stdin\n</code></pre>"},{"location":"applications/openfaas/#openfaas-cli","title":"OpenFaaS-Cli","text":"<pre><code># Mac\nbrew install faas-cli\n</code></pre> <p>First Function</p>"},{"location":"applications/custom/chest-system/","title":"Chest-System","text":"<p>The Chest-System is a system developed by IT-Lab to store and manage items in chests.</p>"},{"location":"applications/custom/chest-system/#installation","title":"Installation","text":"<p>Refers to the <code>chest-system</code> folder</p>"},{"location":"applications/custom/chest-system/#prerequisites","title":"Prerequisites","text":"<p>Note</p> <p>First we need a <code>Postgres</code> database. Please refer to Postgres.</p>"},{"location":"applications/custom/chest-system/#config","title":"Config","text":"<p>Edit the database credentials in the <code>server-deployment.yaml</code> file:</p> <pre><code>- name: POSTGRES_DB\nvalue: chest\n- name: POSTGRES_HOST\nvalue: chest-cluster # We can use the service name without namespace because we are in the same namespace\n- name: POSTGRES_PASSWORD\nvalueFrom:\nsecretKeyRef:\nkey: password\nname: chestuser.chest-cluster.credentials.postgresql.acid.zalan.do # The secret name is composed of: &lt;username&gt;.&lt;cluster-name&gt;.credentials.postgresql.acid.zalan.do\n- name: POSTGRES_USER\nvalue: chestuser\n</code></pre>"},{"location":"applications/custom/chest-system/#deploy","title":"Deploy","text":"<pre><code>kubectl apply -f .\n</code></pre>"},{"location":"applications/custom/sharelatex/","title":"Overleaf / ShareLaTeX","text":"<ul> <li>Create an MongoDB Replica Set by following the instructions in the MongoDB Replica Set section.</li> <li>Create an Redis Standalone by following the instructions in the Redis Standalone section. (We need to create a Redis Standalone because the Redis Cluster is not supported by ShareLaTeX)</li> </ul>","tags":["kubernetes","sharelatex","latex","overleaf"]},{"location":"applications/custom/sharelatex/#installation","title":"Installation","text":"<p>First you need to create a namespace for ShareLaTeX.</p> <pre><code>kubectl create namespace sharelatex-ns\n</code></pre> <p>Then you need to adjust the environment variables in the <code>sharelatex-deployment.yaml</code> file.</p> <p>Set the following environment variables:</p> Variable Description SHARELATEX_MONGO_URL The MongoDB connection string SHARELATEX_REDIS_HOST The Redis host SHARELATEX_EMAIL_FROM_ADDRESS The email address from which the emails are sent SHARELATEX_EMAIL_SMTP_HOST The SMTP host SHARELATEX_EMAIL_SMTP_PORT The SMTP port SHARELATEX_EMAIL_SMTP_SECURE The SMTP secure flag SHARELATEX_EMAIL_SMTP_USER The SMTP user SHARELATEX_EMAIL_SMTP_PASS The SMTP password SHARELATEX_APP_NAME The name of the ShareLaTeX instance SHARELATEX_SITE_URL The URL of the ShareLaTeX instance SHARELATEX_ADMIN_EMAIL The email address of the admin SHARELATEX_BEHIND_PROXY The behind proxy flag SHARELATEX_SECURE_COOKIE The secure cookie flag <p>Also adjust the <code>host</code> in the <code>sharelatex-ingress.yaml</code> file.</p> <p>Then you can deploy ShareLaTeX.</p> <pre><code>kubectl apply -f .\n</code></pre>","tags":["kubernetes","sharelatex","latex","overleaf"]},{"location":"applications/custom/wikijs/","title":"WikiJS","text":"<p>WikiJS is a modern, lightweight and powerful wiki app built on NodeJS, Git and Markdown.</p> <p>Docs</p>"},{"location":"applications/custom/wikijs/#requirements","title":"Requirements","text":"<p>Note</p> <p>First we need a <code>Postgres</code> database. Please refer to Postgres.</p>"},{"location":"applications/custom/wikijs/#install","title":"Install","text":"<p>You can simply use the <code>make</code> command in the <code>Wiki</code> folder to make the config and apply the files</p>"},{"location":"applications/custom/wikijs/#namespace","title":"Namespace","text":"<p>Create the namespace for the application:</p> <pre><code>kubectl create ns wiki-ns\n</code></pre>"},{"location":"applications/custom/wikijs/#config","title":"Config","text":"<p>Edit the database <code>config/config.yml</code> based on the Postgres config.</p> <ul> <li><code>db.host</code>: <code>&lt;postgres-service-name&gt;.&lt;namespace&gt;.svc.cluster.local</code></li> <li><code>db.port</code>: <code>5432</code></li> <li><code>db.user</code>: The user you defined for the database</li> <li><code>db.pass</code>: The password for the user from the secret.</li> </ul> <p>Create the configmap from the file:</p> <pre><code>kubectl create configmap config-configmap --from-file=config/config.yml -n wiki-ns\n</code></pre>"},{"location":"applications/custom/wikijs/#deployment","title":"Deployment","text":"<p>Deploy the application:</p> <pre><code>kubectl apply -f wiki-deployment.yaml,wiki-service.yaml,wiki-ingress.yaml -n wiki-ns\n</code></pre> <p>Access the application: https://wiki.k3s.test</p>"},{"location":"applications/databases/mongodb/","title":"MongoDB","text":"<p>Docs</p> <p>We will use the MongoDB Operator to manage our MongoDB instances. The MongoDB Operator is a Kubernetes Operator that manages the MongoDB instances.</p> <p>In this example we will create a MongoDB Replica Set. We will use it for our ShareLaTeX instance.</p>"},{"location":"applications/databases/mongodb/#installation","title":"Installation","text":"<pre><code>helm repo add mongodb-helm-charts https://mongodb.github.io/helm-charts\n# Install the MongoDB Operator\nhelm install my-community-operator mongodb-helm-charts/community-operator --namespace mongodb --create-namespace --set operator.watchNamespace=\"*\"\n# Deploy a MongoDB Replica Set\nkubectl apply -f replicaset.yaml -n mongodb\n# Check\nkubectl get mdbc -n mongodb\n</code></pre>"},{"location":"applications/databases/mongodb/#connection-string","title":"Connection String","text":"<p>Now you can get the connection string from the secret in the namespace <code>mongodb</code> with the pattern <code>&lt;mongodb-resource-name&gt;-&lt;database&gt;-&lt;username&gt;</code>. In our case it is <code>example-mongodb-admin-my-user</code>.</p> <pre><code>kubectl get secret example-mongodb-admin-my-user -n mongodb -o json | jq -r '.data | with_entries(.value |= @base64d)'\n</code></pre> <p>You can now use the <code>standardSrv</code> connection string to connect to the MongoDB Replica Set. We will use it in our ShareLaTeX instance.</p>"},{"location":"applications/databases/mongodb/#test","title":"Test","text":"<p>Connect with the <code>standardSrv</code> connection string to the MongoDB Replica Set.</p> <pre><code>kubectl run mongopod -i -t --image=rtsp/mongosh -- bash\n&gt; mongosh \"&lt;connection-string&gt;\"\n</code></pre>"},{"location":"applications/databases/postgresql/","title":"Postgres","text":"<p>We will use the Postgres Operator to deploy a Postgres Cluster.</p>"},{"location":"applications/databases/postgresql/#deploy-postgres-operator","title":"Deploy Postgres Operator","text":"<pre><code># add repo for postgres-operator\nhelm repo add postgres-operator-charts https://opensource.zalando.com/postgres-operator/charts/postgres-operator\n\n# install the postgres-operator\nhelm install postgres-operator postgres-operator-charts/postgres-operator\n</code></pre> <p>Check if running</p> <pre><code>kubectl get pod -l app.kubernetes.io/name=postgres-operator\n</code></pre>"},{"location":"applications/databases/postgresql/#deploy-operator-ui","title":"Deploy Operator UI","text":"<pre><code># add repo for postgres-operator-ui\nhelm repo add postgres-operator-ui-charts https://opensource.zalando.com/postgres-operator/charts/postgres-operator-ui\n\n# install the postgres-operator-ui\nhelm install postgres-operator-ui postgres-operator-ui-charts/postgres-operator-ui\n</code></pre> <p>Check if the UI is running</p> <pre><code>kubectl get pod -l app.kubernetes.io/name=postgres-operator-ui\n</code></pre> <p>http://127.0.0.1:8081</p>"},{"location":"applications/databases/postgresql/#create-a-postgres-cluster","title":"Create a Postgres Cluster","text":"<pre><code>kubectl create -f acid-cluster.yaml\n</code></pre> <p>Check if the cluster is running</p> <pre><code># check the deployed cluster\nkubectl get postgresql\n\n# check created database pods\nkubectl get pods -l application=spilo -L spilo-role\n\n# check created service resources\nkubectl get svc -l application=spilo -L spilo-role\n</code></pre>"},{"location":"applications/databases/postgresql/#password","title":"Password","text":"<p>As you can see in the acid-cluster.yaml there are multiple users and multiple databases.</p> <pre><code>users:\nzalando:  # database owner\n- superuser\n- createdb\nfoo_user: []  # role for application foo\ndatabases:\nfoo: zalando  # dbname: owner\npreparedDatabases:\nbar: {}\n</code></pre> <p>The passwords are stored in secrets. The password for the <code>postgres</code> user is stored in a secret. We can get the password with:</p> <pre><code>export PGPASSWORD=$(kubectl get secret postgres.acid-minimal-cluster.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)\necho $PGPASSWORD\n\n# Use encrypted connections\nexport PGSSLMODE=require\n</code></pre> <p>The secret name is composed of: <code>&lt;username&gt;.&lt;cluster-name&gt;.credentials.postgresql.acid.zalan.do</code></p>"},{"location":"applications/databases/postgresql/#for-wikijs","title":"For WikiJS","text":"<pre><code>kubectl create -f wiki-cluster.yaml\n</code></pre> <p>Get the password for the <code>wikiuser</code> user:</p> <pre><code>export PGPASSWORD=$(kubectl get secret wikiuser.wiki-cluster.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)\necho $PGPASSWORD\n</code></pre> <p>The <code>host</code> is: <code>wiki-cluster.default.svc.cluster.local</code></p>"},{"location":"applications/databases/redis/","title":"Redis","text":"<p>Docs</p> <p>First we need the Redis Operator, we install it in the <code>ot-operators</code> namespace. The Redis Operator is a Kubernetes Operator that manages the Redis instances.</p>"},{"location":"applications/databases/redis/#redis-operator-required","title":"Redis Operator (Required)","text":"<pre><code>helm install redis-operator ot-helm/redis-operator --namespace ot-operators --create-namespace\n</code></pre>"},{"location":"applications/databases/redis/#redis-instance","title":"Redis Instance","text":""},{"location":"applications/databases/redis/#standalone","title":"Standalone","text":"<p>We can create a Redis Standalone instance by using the following command: We will use it e.g. for ShareLaTeX.</p> <pre><code>helm install redis ot-helm/redis --namespace ot-operators\n</code></pre> <p>The FQDN to access the redis instance is <code>&lt;name&gt;.&lt;namespace&gt;.svc.cluster.local</code>.</p> <p>So in our case it is <code>redis.ot-operators.svc.cluster.local</code>.</p>"},{"location":"applications/monitoring/","title":"Monitoring via Prometheus and Grafana","text":"<p>The monitoring stack consists of Prometheus and Grafana. It uses a Prometheus Operator to create the Prometheus and Alertmanager instances.</p>"},{"location":"applications/monitoring/alertmanager/","title":"Alertmanager","text":""},{"location":"applications/monitoring/grafana/","title":"Grafana","text":"<p>Deploy Grafana</p> <pre><code>kubectl apply -f grafana/\n</code></pre> <p>Login with <code>admin</code> and <code>admin</code></p> <p>Go to Settings &gt; Data Sources and add the Prometheus Data Source with the following URL: <code>http://prometheus:9090</code></p> <p>Import the dashboard with the ID <code>8171</code> from the Grafana Dashboard Store.</p>"},{"location":"applications/monitoring/prometheus-operator/","title":"Prometheus Operator","text":"<p>The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances.</p> <p>This is a helper that extends the Kubernetes API and allows you to manage Prometheus servers and configurations using Kubernetes native tools and workflows.</p>"},{"location":"applications/monitoring/prometheus-operator/#create-the-namepsace","title":"Create the namepsace","text":"<pre><code>kubectl create namespace monitoring\n</code></pre>"},{"location":"applications/monitoring/prometheus-operator/#install","title":"Install","text":"<p>Use the <code>bundle.yaml</code> in the Prometheus-Operator folder to install the Operator.</p> <pre><code>kubectl apply --server-side -f bundle.yaml\n</code></pre>"},{"location":"applications/monitoring/prometheus-operator/#check-the-deployment","title":"Check the deployment","text":"<pre><code>kubectl get pods -n monitoring\n</code></pre>"},{"location":"applications/monitoring/prometheus/","title":"Prometheus","text":"<p>In the <code>monitoring</code> folder</p> <p>First wie create the config for the additional scrape jobs. The output is written to <code>prometheus/additional-scrape-configs.yaml</code>.</p> <pre><code>kubectl create secret generic additional-scrape-configs -n monitoring --from-file=prometheus/additional-job/prometheus-additional-job.yaml --dry-run=client -oyaml &gt; prometheus/additional-scrape-configs.yaml\n</code></pre> <p>Then we create the Prometheus Instance with the following command:</p> <pre><code>kubectl apply -f prometheus/\n</code></pre> <p>You can also use the <code>make apply-config</code> command in the <code>monitoring</code> folder</p>"},{"location":"applications/monitoring/service-monitor/","title":"Service Monitor","text":"<p>The Files are in seperate folders for each application.</p>"},{"location":"applications/monitoring/service-monitor/#longhorn","title":"Longhorn","text":"<p>Apply the Longhorn Service Monitor</p> <pre><code>kubectl apply -f longhorn/\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#check-the-deamonset","title":"Check the Deamonset","text":"<pre><code>\u279c  kubectl get daemonset -n longhorn-system\n\nNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE\nlonghorn-manager 2 2 2 2 2 &lt;none&gt; 2d19h\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#kube-state-metrics","title":"Kube State Metrics","text":"<p>Kube State Metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects.</p> <p>Apply the Kube State Metrics Service Monitor</p> <pre><code>kubectl apply -f kube-state-metrics/\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#node-exporter","title":"Node-Exporter","text":"<p>The Prometheus Node Exporter exposes a wide variety of hardware- and kernel-related metrics.</p> <p>Apply the Node-Exporter Service Monitor</p> <pre><code>kubectl apply -f node-exporter/\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#kubelet","title":"Kubelet","text":"<p>The Kubelet is the primary \u201cnode agent\u201d that runs on each node. It can register the node with the apiserver using one of: the hostname; a flag to override the hostname; or specific logic for a cloud provider.</p> <p>Apply the Kubelet Service Monitor</p> <pre><code>kubectl apply -f kubelet/\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#traefik","title":"Traefik","text":"<p>Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them.</p> <p>We will also use it to expose the Prometheus and Alertmanager UIs.</p> <p>Apply the Traefik Service Monitor</p> <pre><code>kubectl apply -f traefik/\n</code></pre>"},{"location":"applications/monitoring/service-monitor/#check-the-service-monitors","title":"Check the Service Monitors","text":"<pre><code>\u279c  Monitoring git:(main) \u2717 kubectl get ServiceMonitor -n monitoring\nNAME                                 AGE\nlonghorn-prometheus-servicemonitor   39h\nnode-exporter                        39h\nkube-state-metrics                   39h\nkubelet                              39h\ntraefik                              38h\n</code></pre>"},{"location":"create-cluster/","title":"Create a cluster","text":"<p>In this section we will show you how to create a Kubernetes cluster and configure Traefik.</p>"},{"location":"create-cluster/#choose-a-cluster-type","title":"Choose a cluster type","text":"<p>You can choose between the following cluster types:</p> <ul> <li>Kind</li> <li>K3s</li> <li>K8s</li> </ul>"},{"location":"create-cluster/#configure-traefik","title":"Configure Traefik","text":"<p>After creating the cluster we need to configure Traefik.</p> <p>Configure Traefik</p>"},{"location":"create-cluster/k3s/","title":"K3s","text":"<p>Siehe K3s Docs</p> <p>Inspired by Rpi4Cluster.com</p>"},{"location":"create-cluster/k3s/#master","title":"Master","text":"<pre><code>curl -sfL https://get.k3s.io | sh -\n</code></pre> <p>Use the kubeconfig from <code>/etc/rancher/k3s/k3s.yaml</code> to connect to the cluster.</p>"},{"location":"create-cluster/k3s/#nodes","title":"Nodes","text":"<p>Get token from <code>/var/lib/rancher/k3s/server/node-token</code></p> <pre><code>curl -sfL https://get.k3s.io | K3S_URL=https://172.30.62.166:6443 K3S_TOKEN=&lt;TOKEN&gt;  sh -\n</code></pre>"},{"location":"create-cluster/k3s/#label-the-nodes","title":"Label the Nodes","text":"<pre><code>kubectl label nodes &lt;node-name&gt; node-role.kubernetes.io/worker=worker node-type=worker\n</code></pre>"},{"location":"create-cluster/k8s/","title":"Install K8s","text":"<p>Install packages</p> <pre><code>apt update\napt install apt-transport-https ca-certificates curl gnupg2 software-properties-common -y\n</code></pre> <p>Kernel modules for k8s networking</p> <pre><code>modprobe overlay\nmodprobe br_netfilter\ncat &lt;&lt;EOF | tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n</code></pre> <p>Natting</p> <pre><code>cat &lt;&lt;EOF | tee /etc/sysctl.d/99-kubernetes-cri.conf\nnet.bridge.bridge-nf-call-iptables  = 1\nnet.ipv4.ip_forward                 = 1\nnet.bridge.bridge-nf-call-ip6tables = 1\nEOF\nsysctl --system\n</code></pre> <p>Add containerd keys and repos</p> <pre><code>curl https://download.docker.com/linux/debian/gpg | apt-key add\necho \"deb [arch=amd64] https://download.docker.com/linux/debian buster stable\" &gt; /etc/apt/sources.list.d/docker.list\n</code></pre> <p>Install containerd</p> <pre><code>apt update\napt install containerd.io -y\n</code></pre> <p>Configure containerd</p> <pre><code>containerd config default &gt; /etc/containerd/config.toml\n# set SystemdCgroup to true\nsudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml\n</code></pre> <p>Disable swap!</p> <pre><code>sudo sed -i '/ swap / s/^\\(.*\\)$/#\\1/g' /etc/fstab\nswapoff -a\n</code></pre> <p>Restart containerd</p> <pre><code>systemctl enable containerd\nsystemctl restart containerd\n</code></pre> <p>Add k8s repos and install it</p> <pre><code>curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" &gt; /etc/apt/sources.list.d/kubernetes.list\n# install k8s and hold packags\napt update\napt install kubelet kubeadm kubectl -y\napt-mark hold kubelet kubeadm kubectl\n</code></pre> <p>Switch iptables to legacy</p> <pre><code>update-alternatives --config iptables\n# --&gt; select iptables-legacy\nreboot\n</code></pre>"},{"location":"create-cluster/k8s/#cluster-erstellen","title":"Cluster erstellen:","text":"<p>POD-CIDR -&gt; internal used to give every pod a unique ipv4 CRI Socket -&gt;path to containerd/container runtime socket Control-Plane-Endpoint: for single Control Plane: IP-OF-MASTER:6443, else IP:Port to use a Loadbalancer</p> <pre><code>kubeadm init --pod-network-cidr 10.244.0.0/16 --cri-socket /run/containerd/containerd.sock --control-plane-endpoint=&lt;MASTER_IP&gt;:6443\n</code></pre> <p>Copy kubectl config to home folder</p> <pre><code>mkdir -p $HOME/.kube\nudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre> <p>Setup Canal Network</p> <pre><code>mkdir ~/kube/config -p\ncd ~/kube/config\ncurl https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/canal.yaml -O\nkubectl apply -f canal.yaml\n</code></pre>"},{"location":"create-cluster/kind/","title":"Kind","text":"<p>With Kind we can create a local Kubernetes cluster. This is useful for testing and development.</p> <p>The files are in the <code>kind</code> folder.</p> <pre><code>kind create cluster --name example-cluster --config kind-config.yaml\n</code></pre>"},{"location":"create-cluster/kind/#point-kubectl-to-the-local-cluster","title":"Point kubectl to the local cluster","text":"<p>The context is <code>kind-</code> followed by the cluster <code>name</code>.</p> <pre><code>kubectl cluster-info --context kind-example-cluster\n</code></pre>"},{"location":"create-cluster/kind/#get-the-config","title":"Get the config","text":"<pre><code>kubectl config view --minify --flatten --context kind-example-cluster\n</code></pre>"},{"location":"create-cluster/traefik/","title":"Networking - Traefik","text":"<p>Simply use <code>kubectl apply -f .</code> in the <code>Traefik</code> folder to apply all files.</p> <p>We configure traefike to do the following:</p> <ul> <li>Expose the dashboard on port <code>9000</code></li> <li>Middleware to redirect <code>http</code> to <code>https</code></li> <li>Middleware for basic auth</li> <li>Middleware for cors</li> </ul> <p>Refernce</p>"},{"location":"create-cluster/traefik/#expose-the-dashboard","title":"Expose the Dashboard","text":"<p><code>traefik.yaml</code></p> <pre><code>apiVersion: helm.cattle.io/v1\nkind: HelmChartConfig\nmetadata:\nname: traefik\nnamespace: kube-system\nspec:\nvaluesContent: |-\nadditionalArguments:\n- \"--api\"\n- \"--api.dashboard=true\"\n- \"--api.insecure=true\"\n- \"--log.level=DEBUG\"\nports:\ntraefik:\nexpose: true\nproviders:\nkubernetesCRD:\nallowCrossNamespace: true\n</code></pre> <p><code>kubectl apply -f traefik.yaml</code></p> <p>Expose Services</p> <p>http://172.30.62.166:9000/dashboard/#/</p>"},{"location":"create-cluster/traefik/#basic-auth","title":"Basic Auth","text":"<p>First we need an Traefik Middleware. We define it in the default namespace, because we will use it in all namespaces. The credentials are <code>admin:admin</code> and are base64 encoded.</p> <p>You can encode your own credentials with:</p> <pre><code>echo -n 'admin' | base64\n</code></pre> <p>Create the Middleware:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\nname: my-auth\nnamespace: default\ntype: kubernetes.io/basic-auth\ndata:\nusername: YWRtaW4=\npassword: YWRtaW4=\n---\napiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\nname: my-auth-middleware\nnamespace: default\nspec:\nbasicAuth:\nremoveHeader: true\nsecret: my-auth\n</code></pre>"},{"location":"create-cluster/traefik/#use-the-middleware-in-the-ingress-annotation","title":"Use the Middleware in the Ingress annotation:","text":"<p><code>&lt;namespace&gt;-&lt;middleware-name&gt;@kubernetescrd</code></p> <p><code>default-my-auth-middleware@kubernetescrd</code></p> <p>Use at the end of the list =&gt; Redirect to HTTPS before authentication</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\nname: web-ingress\nnamespace: linkerd-viz\nannotations:\nkubernetes.io/ingress.class: traefik\ntraefik.ingress.kubernetes.io/router.middlewares: default-cors@kubernetescrd,default-redirectscheme@kubernetescrd,default-my-auth-middleware@kubernetescrd\nspec:\nrules:\n- host: linkerd.k3s.test\nhttp:\npaths:\n- path: /\npathType: Prefix\nbackend:\nservice:\nname: web\nport:\nnumber: 8084\n</code></pre>"},{"location":"create-cluster/traefik/#cors","title":"CORS","text":"<pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\nname: cors\nnamespace: default\nspec:\nheaders:\naccessControlAllowOriginList:\n- 'https://hub.k3s.it-lab.cc'\naccessControlAllowMethods:\n- HEAD\n- GET\n</code></pre>"},{"location":"create-cluster/traefik/#redirect-http-to-https","title":"Redirect HTTP to HTTPS","text":"<pre><code>apiVersion: traefik.containo.us/v1alpha1\nkind: Middleware\nmetadata:\nname: redirectscheme\nnamespace: default\nspec:\nredirectScheme:\nscheme: https\npermanent: true\n</code></pre>"},{"location":"create-cluster/traefik/external-dns/","title":"External dns","text":""},{"location":"create-cluster/traefik/external-dns/#config","title":"Config","text":"<p><code>&lt;email&gt;</code> and <code>domain-filer</code> in <code>Traefik/external-dns/values.yaml</code>:</p> <pre><code>env:\n- name: CF_API_KEY\nvalueFrom:\nsecretKeyRef:\nname: cloudflare-api-key\nkey: api-key\n- name: CF_API_EMAIL\nvalue: '&lt;email&gt;'\n</code></pre> <pre><code>extraArgs:\n- --domain-filter=&lt;your-domain&gt;\n</code></pre> <p>Both <code>&lt;api-key&gt;</code> in <code>Traefik/cf-api-secret.yaml</code>:</p> <pre><code>stringData:\napi-key: '&lt;api-key&gt;'\n</code></pre> <p><code>&lt;email&gt;</code> in <code>Traefik/traefik-config.yaml</code>:</p> <pre><code>env:\n- name: CF_API_EMAIL\nvalue: '&lt;email&gt;'\n</code></pre> <pre><code># Create the external-dns namespace\nkubectl create namespace external-dns\n# Create the Cloudflare API Key\nkubectl apply -f cf-api-secret.yaml\n# Apply the config for traefik from TLS\nkubectl apply -f traefik-config.yaml\n\n\n# Cert-Manager\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.13.0/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nhelm install cert-manager --namespace cert-manager --version v1.13.0 jetstack/cert-manager --create-namespace\n\n# External-DNS\nhelm repo add external-dns https://kubernetes-sigs.github.io/external-dns/\nhelm upgrade --install external-dns external-dns/external-dns --values external-dns/values.yaml --namespace external-dns\n</code></pre> <p><code>annotations</code> in your <code>Ingress</code> definitions:</p> <pre><code>annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure\ntraefik.ingress.kubernetes.io/router.tls: 'true'\ntraefik.ingress.kubernetes.io/router.tls.certresolver: myresolver\n</code></pre> <p>Siehe:</p> <ul> <li>https://kubernetes-sigs.github.io/external-dns/v0.12.2/tutorials/hostport/#kafka-stateful-set</li> <li>proxied</li> </ul>"},{"location":"create-cluster/traefik/tls/","title":"Traefik - TLS","text":"<p>We will configure Traefik to use Letsencrypt <code>DNS Challenges</code> to automatically generate TLS certificates for our services.</p>"},{"location":"create-cluster/traefik/tls/#prerequisites","title":"Prerequisites","text":"<p>We need an account with a DNS provider that supports the ACME DNS Challenge and a domain name.</p> <p>We will use Cloudflare as our DNS provider.</p>"},{"location":"create-cluster/traefik/tls/#cloudflare","title":"Cloudflare","text":"<p>Get the Global API Key from your Cloudflare account. Get your \"Global API Key\"</p>"},{"location":"create-cluster/traefik/tls/#traefik","title":"Traefik","text":"<p>Now we need to configure Traefik to use the Cloudflare DNS Challenge. Refer to the <code>traefik/traefik-config.yml</code> file.</p> <p>Uncomment the <code>additionalArguments</code> for the resolver and replace the <code>&lt;email&gt;</code> with your email address.</p> <pre><code>- '--certificatesresolvers.myresolver.acme.dnschallenge=true'\n- '--certificatesresolvers.myresolver.acme.dnschallenge.provider=cloudflare'\n- '--certificatesresolvers.myresolver.acme.email=&lt;email&gt;'\n- '--certificatesresolvers.myresolver.acme.storage=/data/acme.json'\n</code></pre> <p>Then uncomment the <code>env</code> for the Cloudflare API Key and replace the <code>&lt;email&gt;</code> and <code>&lt;key&gt;</code> with your Email and Global API Key.</p> <pre><code>env:\n- name: CF_API_EMAIL\nvalue: '&lt;email&gt;'\n- name: CF_API_KEY\nvalue: '&lt;api_key&gt;' #&lt;== Unter https://dash.cloudflare.com/profile/api-tokens \"Global API Key\"\n</code></pre>"},{"location":"create-cluster/traefik/tls/#deploy-traefik","title":"Deploy Traefik","text":"<p>Deploy Traefik with the new configuration.</p> <pre><code>kubectl apply -f traefik-config.yaml\n</code></pre>"},{"location":"create-cluster/traefik/tls/#usage","title":"Usage","text":"<p>Now we can use the <code>myresolver</code> in our <code>Ingress</code> definitions.</p> <pre><code>annotations:\ntraefik.ingress.kubernetes.io/router.entrypoints: websecure\ntraefik.ingress.kubernetes.io/router.tls: 'true'\ntraefik.ingress.kubernetes.io/router.tls.certresolver: myresolver\n</code></pre> <p>Please note that the <code>host</code> in the <code>Ingress</code> definition must match the domain name.</p>"},{"location":"create-cluster/traefik/tls/#trouble-shooting","title":"Trouble Shooting","text":"<p>Check the logs of the Traefik pod.</p> <pre><code>kubectl logs deploy/traefik -n kube-system\n</code></pre> <p>Pay attention to the name of the <code>certresolver</code>. The name in the <code>traefik-config.yml</code> must match the name in the <code>Ingress</code> definition.</p>"},{"location":"random/coredns/","title":"CoreDNS","text":"<p>CoreDNS always appends the domain 'it-lab.cc' to the hostname. This is not wanted in our case. Therefore we have to change the CoreDNS config.</p> <p>(Did not found a better solution)</p> <p>We can use the rewrite Plugin to remove the <code>.it-lab.cc</code> from the hostname.</p> <p><code>coredns-cm.yaml</code> in <code>dns</code> folder</p> <pre><code>rewrite name substring .com.it-lab.cc .com\n</code></pre> <p>Apply the config by running <code>make</code> in the <code>dns</code> folder</p>"},{"location":"storage/","title":"Storage","text":"<p>This is a collection of storage solutions for k3s.</p> <p>NFS is used for the production environment. Longhorn is more for testing purposes.</p> <p>You can use the StorageClass <code>local-path</code> for testing purposes. It is not recommended for production use.</p>"},{"location":"storage/longhorn/","title":"Longhorn","text":"<p>Siehe Longhorn Docs Longhorn Examples</p>"},{"location":"storage/longhorn/#requirements","title":"Requirements","text":"<pre><code>apt install jq nfs-common -y\n</code></pre> <p>Check Script</p> <pre><code>curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.5.1/scripts/environment_check.sh | bash\n</code></pre>"},{"location":"storage/longhorn/#install","title":"Install","text":"<p>Use <code>make</code> in <code>Longhorn</code> folder</p> <pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\nhelm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.5.1\n</code></pre> <p>Check deployment (5-10 Minutes), till all pods are up</p> <pre><code>kubectl -n longhorn-system get pod\n</code></pre> <p>Make the Longhorn accessable via Web Dashboard</p>"},{"location":"storage/nfs/","title":"NFS Storage","text":"<pre><code>helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\n\nhelm install nfs-subdir-external-provisioner --namespace nfs-subdir-external-provisioner --create-namespace nfs-subdir-external-provisioner/nfs-subdir-external-provisioner --set nfs.server=172.30.41.5 --set nfs.path=/k3s-test-sj\n</code></pre> <p>Set NFS as default</p> <pre><code># Mark local-path as non-default\n\nkubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}'\n\n# Mark nfs-client as default\n\nkubectl patch storageclass nfs-client -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"true\"}}}'\n\nkubectl get storageclass\n</code></pre>"},{"location":"tools/","title":"Tools","text":"<p>In this section you will find a list of tools that can be used to help you with your development.</p> <p>It also contains a list of tools for Continuous Deployment.</p>"},{"location":"tools/dashboard/","title":"Kubernetes-Dashboard","text":"<p>I prefer to use the Lens Dashboard instead of the Kubernetes Dashboard.</p> <p>Use <code>make</code> in <code>Dashboard</code> folder</p> <p>Siehe Helm Hub</p> Click for Installation <pre><code>helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/\nhelm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard\n\nexport POD_NAME=$(kubectl get pods -n kubernetes-dashboard -l \"app.kubernetes.io/name=kubernetes-dashboard,app.kubernetes.io/instance=kubernetes-dashboard\" -o jsonpath=\"{.items[0].metadata.name}\")\nkubectl expose pod $POD_NAME --port=8443 --target-port=8443 --type=LoadBalancer -n kubernetes-dashboard\n</code></pre>  `dashboard.admin-user.yml`  <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\nname: admin-user\nnamespace: kubernetes-dashboard\n</code></pre>  `dashboard.admin-user-role.yml`  <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\nname: admin-user\nroleRef:\napiGroup: rbac.authorization.k8s.io\nkind: ClusterRole\nname: cluster-admin\nsubjects:\n\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n</code></pre>  Deploy the configs  <pre><code>sudo k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml\n</code></pre>"},{"location":"tools/dashboard/#get-the-token","title":"Get the Token","text":"<pre><code>kubectl -n kubernetes-dashboard create token admin-user --duration=24h\n</code></pre> <p>https://172.30.62.166:8443/#/login</p>"},{"location":"tools/helm%2Barkade/","title":"Helm","text":"<p>Siehe Helm Docs</p>"},{"location":"tools/helm%2Barkade/#install","title":"Install","text":"<pre><code>curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg &gt; /dev/null\nsudo apt-get install apt-transport-https --yes\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\nsudo apt-get update\nsudo apt-get install helm\n</code></pre>"},{"location":"tools/helm%2Barkade/#arkade","title":"Arkade","text":"<pre><code># Linux\ncurl -sLS https://get.arkade.dev | sudo sh\n# Mac\nbrew install arkade\n\n# Check version\narkade version\n</code></pre>"},{"location":"tools/linkerd/","title":"Linkerd","text":"<p>Linkerd is a service mesh for Kubernetes and other frameworks. It makes running services easier and safer by giving you runtime debugging, observability, reliability, and security\u2014all without requiring any changes to your code.</p>"},{"location":"tools/linkerd/#cli","title":"CLI","text":"<pre><code># Mac\nbrew install linkerd\n</code></pre>"},{"location":"tools/linkerd/#install-in-cluster","title":"Install in cluster","text":"<pre><code>linkerd check --pre\nlinkerd install --crds | kubectl apply -f -\nlinkerd install | kubectl apply -f -\nlinkerd check\n</code></pre>"},{"location":"tools/linkerd/#run-a-demo-app","title":"Run a Demo App","text":"<pre><code>curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/emojivoto.yml | kubectl apply -f -\n\n# Expose (for reference)\nkubectl -n emojivoto port-forward svc/web-svc 8080:80\n</code></pre> <p>http://127.0.0.1:8080</p> <p>Try to vote for the donut emoji, you\u2019ll get a 404 page.</p> <p>Don\u2019t worry, these errors are intentional. (In a later guide, we\u2019ll show you how to use Linkerd to identify the problem.)</p> <p>Inject Linkerd</p> <pre><code>kubectl get -n emojivoto deploy -o yaml \\\n| linkerd inject - \\\n| kubectl apply -f -\n\n# Check the data planes\nlinkerd -n emojivoto check --proxy\n</code></pre>"},{"location":"tools/linkerd/#viz","title":"Viz","text":"<pre><code>linkerd viz install --set dashboard.enforcedHostRegexp=\".*\" | kubectl apply -f - # install the on-cluster metrics stack\nlinkerd check\n\nlinkerd viz dashboard &amp;\n</code></pre> <p>You can now see the dashboard and navigate to the emojivoto namespace.</p> <p>You should see the <code>web</code> deployment with a <code>Success Rate</code> below 100%.</p>"},{"location":"tools/linkerd/#expose-permanent","title":"Expose Permanent","text":"<pre><code>kubectl apply -f linkerd\n</code></pre>"},{"location":"tools/rancher/","title":"Rancher (Not used)","text":"<p>Replace <code>IP_OF_LINUX_NODE</code></p> <pre><code>helm repo add rancher-latest https://releases.rancher.com/server-charts/latest\nkubectl create namespace cattle-system\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.crds.yaml\nhelm repo add jetstack https://charts.jetstack.io\nhelm repo update\nhelm install cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.11.0\n</code></pre> <p>Replace <code>IP_OF_LINUX_NODE</code> and <code>bootstrapPassword</code></p> <pre><code>helm install rancher rancher-latest/rancher \\\n  --namespace cattle-system \\\n  --set hostname=IP_OF_LINUX_NODE.sslip.io \\\n  --set replicas=1 \\\n  --set bootstrapPassword=Admin123\n</code></pre> <p>Set <code>IP_OF_LINUX_NODE.sslip.io</code> to the IP of the Linux Node in your network or in the <code>/etc/hosts</code> file.</p> <p>https://172.30.62.166.sslip.io</p>"},{"location":"tools/continuous-deployment/","title":"Deployment","text":"<p>There are several ways to deploy applications to our cluster. We can use the cli <code>kubectl</code> or a package manager like <code>helm</code> or <code>arkade</code>.</p> <p>For continuous deployment via <code>git</code> we can use <code>ArgoCD</code>. <code>ArgoCD</code> has a nice UI and is easy to use. It syncs with git and deploys the applications to the cluster.</p> <p>Another way to deploy applications is with <code>terraform</code>. This is a good way to create services on the cluster. For example a database or a storage service. It is also possible to deploy applications with <code>terraform</code>. It can also be used to create the cluster itself in the cloud.</p>"},{"location":"tools/continuous-deployment/argocd/","title":"ArgoCD","text":"<p>ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. You can deploy applications to Kubernetes using GitOps principles.</p>"},{"location":"tools/continuous-deployment/argocd/#install-cli","title":"Install CLI","text":"<pre><code>brew install argocd\n</code></pre>"},{"location":"tools/continuous-deployment/argocd/#install-in-cluster","title":"Install in cluster","text":"<pre><code>kubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n# In ArgoCD Folder\nkubectl apply -f configmap.yaml\nkubectl apply -f ingress.yaml\n# Restart the argocd-server deployment\nkubectl rollout restart deployment argocd-server -n argocd\n# Get the password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d; echo\n</code></pre> <p>https://argocd.k3s.test/</p> <p>Now you can login with user <code>admin</code> and the password from above. You should then change the password.</p>"},{"location":"tools/continuous-deployment/terraform/","title":"Terraform","text":"<p>With Terraform we can create services on the cluster.</p> <p>Now we can connect terraform to the Cluster. First we need to get the config for the cluster with kubectl.</p> <pre><code>kubectl config view --minify --flatten\n</code></pre> <p>This will give us the config to connect to the cluster. We need to add this to the terraform file.</p> <p>Fill these values in the <code>terraform.tfvars</code> file.</p> <pre><code>host = clusters.cluster.server\nclient_certificate = users.user.client-certificate\nclient_key = users.user.client-key\ncluster_ca_certificate = clusters.cluster.certificate-authority-data\n\n# Example:\nhost                   = \"https://127.0.0.1:32768\"\nclient_certificate     = \"LS0tLS1CRUdJTiB...\"\nclient_key             = \"LS0tLS1CRUdJTiB...\"\ncluster_ca_certificate = \"LS0tLS1CRUdJTiB...\"\n</code></pre>"},{"location":"tools/continuous-deployment/terraform/#example-homer","title":"Example: Homer","text":"<p>In the <code>homer</code> folder we have an example of how to use terraform to create a service on the cluster.</p> <p>First we need to declare the <code>terraform.tfvars</code> file. We can copy the <code>terraform.tfvars.example</code> file and fill in the values.</p> <pre><code>cp terraform.tfvars.example terraform.tfvars\nvim terraform.tfvars\n</code></pre> <pre><code>Then we can run terraform.\n</code></pre> <pre><code>terraform apply\n</code></pre>"},{"location":"tools/continuous-deployment/terraform/#config","title":"Config","text":"<p>The <code>configmap.tf</code> file holds the configmap of the service. It reads the <code>homer-config.yaml</code> file in the folder. The <code>deployment.tf</code> file holds the deployment of the service. It mounts the configmap as a volume.</p> <p>The <code>terraform.tfvars</code> file holds the variables for the connection to the cluster. The <code>provider.tf</code> file holds the connection to the cluster. The <code>service.tf</code> file holds the service of the service.</p>"},{"location":"usage/auto-completion/","title":"Auto Completion","text":""},{"location":"usage/auto-completion/#mac-zsh","title":"Mac ZSH","text":"<pre><code>brew install bash-completion\necho 'source &lt;(kubectl completion zsh)' &gt;&gt;~/.zshrc\nsource ~/.zshrc\n# Alias\necho 'alias k=kubectl' &gt;&gt;~/.zshrc\n</code></pre> <p>Now you can use <code>k</code> instead of <code>kubectl</code>. And you can use <code>&lt;TAB&gt;</code> to auto complete commands.</p>"},{"location":"usage/controll/","title":"Start/Stop Containers","text":"<p>Start Objects</p> <pre><code>kubectl apply -f folder/\n</code></pre> <p>Delete Objects</p> <p>Also deletes the storage</p> <pre><code>kubectl delete -f folder/\n</code></pre> <p>\"Stops\" the Containers. (Real stop is not possible in k8s)</p> <pre><code>kubectl scale --replicas=0 -f kube/client-deployment.yaml,kube/db-deployment.yaml,kube/server-deployment.yaml,kube/proxy-deployment.yaml\n</code></pre>"},{"location":"usage/expose/","title":"Expose","text":""},{"location":"usage/expose/#temporary","title":"Temporary:","text":"<pre><code>kubectl port-forward pods/mongo-75f59d57f4-4nd6q 28015:27017\n</code></pre>"},{"location":"usage/expose/#permanently","title":"Permanently:","text":"<pre><code>kubectl expose deployment &lt;deployment_name&gt; --port=8765 --target-port=9376 --name=example-service --type=LoadBalancer\n</code></pre> <p>You can also replace deployment with <code>pod</code></p> <p><code>target-port</code> = in Container</p>"},{"location":"usage/local-connection/","title":"Local connection","text":""},{"location":"usage/local-connection/#mac","title":"Mac","text":"<pre><code>brew install kubectl\nbrew install helm\n</code></pre> <p>Copy the Config from the <code>Manager Node</code> at <code>cat /etc/rancher/k3s/k3s.yaml</code> to your lokal PC <code>~/.kube/config</code></p> <p>Change the <code>server</code> in the config to <code>https://172.30.62.166:6443</code></p> <p>Test the connection:</p> <pre><code>kubectl cluster-info dump\n</code></pre>"},{"location":"usage/networking/","title":"Networking with Traefik","text":"<ul> <li>Service to proxy deployment (via selector <code>io.kompose.service: proxy</code>)</li> <li>Middleware for auto redirection https in <code>default</code> namespace (<code>allowCrossNamespace: true</code> must be set).</li> <li>Ingress creates an Traefik Object, which is connected to the service. <code>web</code> and <code>websecure</code> as entrypoint and <code>middleware</code> for redirection.</li> </ul> <pre><code>sequenceDiagram\n    participant Client\n    box Grey Kubernetes Cluser\n    participant Traefik\n    participant Ingress\n    participant Service\n    participant Deployment\n    participant Pod\n    end\n    Client-&gt;&gt;Traefik: Request (whomai.k3s.it-lab.cc)\n    Traefik--&gt;&gt;Ingress: Handle Request\n    Ingress--&gt;&gt;Service: Map URL to Service (http://whoami)\n    Service--&gt;&gt;Deployment: Map Service to Deployment (whoami)\n    Deployment--&gt;&gt;Pod: Choose Pod\n    Pod-&gt;&gt;Client: Response (whoami)</code></pre> <p>Ingress Template</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name:\n  namespace:\n  annotations:\n    traefik.ingress.kubernetes.io/router.middlewares: default-cors@kubernetescrd,default-redirectscheme@kubernetescrd\nspec:\n  rules:\n    - host: example.k3s.test\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name:\n                port:\n                  number:\n</code></pre>"},{"location":"usefull-commands/Manifests/","title":"Manifests","text":""},{"location":"usefull-commands/Manifests/#create-a-manifest-from-a-docker-image","title":"Create a Manifest from a Docker Image","text":"<pre><code>kubectl create deployment --image=nginx nginx --dry-run=client -o yaml &gt; nginx-deployment.yaml\n</code></pre>"},{"location":"usefull-commands/troubleshooting/","title":"Troubleshooting","text":""},{"location":"usefull-commands/troubleshooting/#run-a-curl-pod","title":"Run a curl Pod","text":"<pre><code>kubectl run mycurlpod --image=curlimages/curl -i --tty -- sh\n</code></pre>"},{"location":"usefull-commands/troubleshooting/#dns-lookup","title":"Dns Lookup","text":"<pre><code>kubectl apply -f https://k8s.io/examples/admin/dns/dnsutils.yaml\nkubectl exec -i -t dnsutils -- nslookup kubernetes.default\n</code></pre>"}]}